# -*- coding: utf-8 -*-
"""sft_eval_diffusion_with_generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i98k0O7duf4YMYDyvJzU-I9HnD14VALb
"""

"""
Fine-tune and evaluate a diffusion-based language model (LLaDA) for Java code refinement.

This script performs:
1. Fine-tuning using supervised fine-tuning (SFT) with LoRA.
2. Evaluation on a held-out test set, including:
   - Generation of revised code.
   - Computation of edit distance and BLEU score.

Author: Victoria Shi et al.
"""

import os
import json
import re
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from nltk.tokenize import word_tokenize
import Levenshtein

from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, EarlyStoppingCallback
)
from trl import SFTTrainer
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig

# ----------------------------
# CONFIGURATION
# ----------------------------

MODEL_ID = "GSAI-ML/LLaDA-8B-Instruct"
OUTPUT_DIR = "gdrive/MyDrive/6.C511Data/llada-sft-final-model-generate-2"
TRAIN_SRC = "gdrive/MyDrive/6.C511Data/1-encoder/train/src-train.txt"
TRAIN_TGT = "gdrive/MyDrive/6.C511Data/1-encoder/train/tgt-train.txt"
VAL_SRC = "gdrive/MyDrive/6.C511Data/1-encoder/test/src-test.txt"
VAL_TGT = "gdrive/MyDrive/6.C511Data/1-encoder/test/tgt-test.txt"
EVAL_SRC = "gdrive/MyDrive/6.C511Data/1-encoder/eval/src-val.txt"
EVAL_TGT = "gdrive/MyDrive/6.C511Data/1-encoder/eval/tgt-val.txt"

MAX_LENGTH = 256
MAX_EVAL_SAMPLES = 100

# ----------------------------
# DATASET PREPARATION
# ----------------------------

def load_text_pairs(src_file, tgt_file):
    with open(src_file, 'r') as f_src, open(tgt_file, 'r') as f_tgt:
        src = f_src.readlines()
        tgt = f_tgt.readlines()
    return Dataset.from_dict({"src": src, "tgt": tgt})

def format_for_sft(example):
    """Format each example for SFT training."""
    prompt = f"Instruction: Please revise the following Java method.\nInput:\n{example['src'].strip()}\nOutput:"
    completion = example["tgt"].strip()
    full_text = prompt + " " + completion
    tokenized = tokenizer(full_text, truncation=True, padding="max_length", max_length=MAX_LENGTH)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

# ----------------------------
# MODEL SETUP
# ----------------------------

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, device_map="auto", load_in_8bit=True, trust_remote_code=True
)

lora_config = LoraConfig(
    r=8, lora_alpha=16,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05, inference_mode=False, task_type="CAUSAL_LM"
)
model = get_peft_model(base_model, lora_config)

# ----------------------------
# TRAINING
# ----------------------------

train_dataset = load_text_pairs(TRAIN_SRC, TRAIN_TGT).select(range(1000)).map(format_for_sft)
val_dataset = load_text_pairs(VAL_SRC, VAL_TGT).select(range(200)).map(format_for_sft)

training_args = TrainingArguments(
    output_dir="gdrive/MyDrive/6.C511Data/sft-generate-llada-lora-checkpoints",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    eval_strategy="steps", eval_steps=125, save_strategy="steps",
    load_best_model_at_end=True, metric_for_best_model="eval_loss", greater_is_better=False,
    logging_steps=10, report_to="none"
)

class LossComputingTrainer(SFTTrainer):
    """Custom trainer to compute masked cross-entropy loss."""
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        active_loss = labels != -100
        active_logits = logits[active_loss]
        active_labels = labels[active_loss]
        loss = F.cross_entropy(active_logits, active_labels)
        return (loss, outputs) if return_outputs else loss

trainer = LossComputingTrainer(
    model=model, args=training_args,
    train_dataset=train_dataset, eval_dataset=val_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer.train()
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("Training complete and model saved.")

# ----------------------------
# EVALUATION
# ----------------------------

config = PeftConfig.from_pretrained(OUTPUT_DIR)
base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path, device_map="auto", load_in_8bit=True, trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)
model.eval()

# Load evaluation data
with open(EVAL_SRC, "r") as f_src, open(EVAL_TGT, "r") as f_tgt:
    src_lines = [line.strip() for line in f_src.readlines()[:MAX_EVAL_SAMPLES]]
    tgt_lines = [line.strip() for line in f_tgt.readlines()[:MAX_EVAL_SAMPLES]]
pairs = list(zip(src_lines, tgt_lines))

def extract_clean_code_from_natural_text(pred):
    """Extract clean Java code block from generated output text."""
    pred = pred.strip()
    if pred.startswith("```"):
        pred = "\n".join(pred.strip().split("\n")[1:]).strip()
    return pred

predictions, references, edit_distances = [], [], []

for src, tgt in tqdm(pairs):
    prompt = f"Instruction: Please revise the following Java method.\nInput:\n{src}\nOutput:"
    m = [{"role": "user", "content": prompt}]
    prompt_str = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)
    input_ids = torch.tensor(tokenizer(prompt_str)['input_ids']).unsqueeze(0)

    with torch.no_grad():
        output_ids = model.generate(input_ids, max_new_tokens=128)
    pred = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
    cleaned = extract_clean_code_from_natural_text(pred)

    predictions.append(cleaned)
    references.append(tgt)
    edit_distances.append(Levenshtein.distance(tgt, cleaned))

# ----------------------------
# METRICS
# ----------------------------

results = {
    "predictions": predictions,
    "references": references,
    "edit_distances": edit_distances,
}
with open("/content/gdrive/MyDrive/sft_eval_outputs_cleaned.json", "w") as f:
    json.dump(results, f, indent=4)

print(f"Results saved to sft_eval_outputs_cleaned.json")

refs_tok = [[word_tokenize(ref)] for ref in references]
hyps_tok = [word_tokenize(pred) for pred in predictions]
smooth_fn = SmoothingFunction().method2
bleu = corpus_bleu(refs_tok, hyps_tok, smoothing_function=smooth_fn)

print(f"Corpus BLEU Score: {bleu * 100:.2f}")
print(f"Average Edit Distance: {np.mean(edit_distances):.2f}")